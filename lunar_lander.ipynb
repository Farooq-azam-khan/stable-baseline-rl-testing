{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym \r\n",
        "from gym.wrappers import RecordVideo"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766370592
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'LunarLander-v2'"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766371869
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)\r\n",
        "env = RecordVideo(env, f'{env_name}-video', episode_trigger=lambda _: True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/batch/tasks/shared/LS_root/mounts/clusters/gpucomputer/code/Users/fkhan/RL-reserach/LunarLander-v2-video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n  logger.warn(\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766373035
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_environment():\r\n",
        "    episodes = 10\r\n",
        "    observation = env.reset()\r\n",
        "    for episode in range(episodes): \r\n",
        "        state = env.reset()\r\n",
        "        done = False \r\n",
        "        score = 0 \r\n",
        "        while not done: \r\n",
        "            action = env.action_space.sample()\r\n",
        "            observation, reward, done, info = env.step(action)\r\n",
        "            score += reward \r\n",
        "        print(f'Episode={episode+1} Score:{score}')\r\n",
        "    env.close()"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766375876
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt install xvfb -y\r\n",
        "#!pip install pyvirtualdisplay\r\n",
        "#!pip install pyglet==1.5.27\r\n",
        "#!sudo apt-get install python-opengl"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672765993359
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1400, 900))\r\n",
        "display.start()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "<pyvirtualdisplay.display.Display at 0x7f868d8a71f0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766380409
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_environment()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Episode=1 Score:-48.609692255247666\nEpisode=2 Score:-193.35645357890647\nEpisode=3 Score:-295.60627897407943\nEpisode=4 Score:-224.99726385081345\nEpisode=5 Score:-415.1097007209371\nEpisode=6 Score:-106.63484597777428\nEpisode=7 Score:-82.17212149868737\nEpisode=8 Score:-217.2607177953961\nEpisode=9 Score:-315.9504930228155\nEpisode=10 Score:-227.6410545009149\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766391366
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import A2C\r\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv "
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766397417
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)\r\n",
        "#env = RecordVideo(env, f'{env_name}-video', episode_trigger=lambda _: True)\r\n",
        "model_name = 'a2c'\r\n",
        "model = A2C('MlpPolicy', env, verbose=1)\r\n",
        "#model = A2C.load(f'{env_name}-{model_name}.zip')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using cuda device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766432805
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=25000)\r\n",
        "model.save(f\"{env_name}-a2c\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | -446     |\n| time/                 |          |\n|    fps                | 267      |\n|    iterations         | 100      |\n|    time_elapsed       | 1        |\n|    total_timesteps    | 500      |\n| train/                |          |\n|    entropy_loss       | -1.3     |\n|    explained_variance | -0.07    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 99       |\n|    policy_loss        | 2.55     |\n|    value_loss         | 19.3     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | -358     |\n| time/                 |          |\n|    fps                | 308      |\n|    iterations         | 200      |\n|    time_elapsed       | 3        |\n|    total_timesteps    | 1000     |\n| train/                |          |\n|    entropy_loss       | -1.22    |\n|    explained_variance | 0.0249   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 199      |\n|    policy_loss        | -5.14    |\n|    value_loss         | 51.4     |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 115       |\n|    ep_rew_mean        | -345      |\n| time/                 |           |\n|    fps                | 311       |\n|    iterations         | 300       |\n|    time_elapsed       | 4         |\n|    total_timesteps    | 1500      |\n| train/                |           |\n|    entropy_loss       | -1.3      |\n|    explained_variance | -7.31e-05 |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 299       |\n|    policy_loss        | -17.6     |\n|    value_loss         | 304       |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 124      |\n|    ep_rew_mean        | -378     |\n| time/                 |          |\n|    fps                | 314      |\n|    iterations         | 400      |\n|    time_elapsed       | 6        |\n|    total_timesteps    | 2000     |\n| train/                |          |\n|    entropy_loss       | -1.24    |\n|    explained_variance | 0.00286  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 399      |\n|    policy_loss        | -37.6    |\n|    value_loss         | 2.37e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 126      |\n|    ep_rew_mean        | -387     |\n| time/                 |          |\n|    fps                | 323      |\n|    iterations         | 500      |\n|    time_elapsed       | 7        |\n|    total_timesteps    | 2500     |\n| train/                |          |\n|    entropy_loss       | -1.09    |\n|    explained_variance | -0.00506 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 499      |\n|    policy_loss        | -2.59    |\n|    value_loss         | 7.23     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 135      |\n|    ep_rew_mean        | -354     |\n| time/                 |          |\n|    fps                | 327      |\n|    iterations         | 600      |\n|    time_elapsed       | 9        |\n|    total_timesteps    | 3000     |\n| train/                |          |\n|    entropy_loss       | -1.03    |\n|    explained_variance | -0.00075 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 599      |\n|    policy_loss        | -2.72    |\n|    value_loss         | 4.76     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 142      |\n|    ep_rew_mean        | -346     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 700      |\n|    time_elapsed       | 10       |\n|    total_timesteps    | 3500     |\n| train/                |          |\n|    entropy_loss       | -0.773   |\n|    explained_variance | 0.000336 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 699      |\n|    policy_loss        | -6.08    |\n|    value_loss         | 164      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 143      |\n|    ep_rew_mean        | -339     |\n| time/                 |          |\n|    fps                | 332      |\n|    iterations         | 800      |\n|    time_elapsed       | 12       |\n|    total_timesteps    | 4000     |\n| train/                |          |\n|    entropy_loss       | -0.671   |\n|    explained_variance | 8.03e-05 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 799      |\n|    policy_loss        | -6.7     |\n|    value_loss         | 23.3     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 147      |\n|    ep_rew_mean        | -323     |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 900      |\n|    time_elapsed       | 13       |\n|    total_timesteps    | 4500     |\n| train/                |          |\n|    entropy_loss       | -0.872   |\n|    explained_variance | 0.000809 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 899      |\n|    policy_loss        | 1.33     |\n|    value_loss         | 4.97     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 153      |\n|    ep_rew_mean        | -309     |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 1000     |\n|    time_elapsed       | 14       |\n|    total_timesteps    | 5000     |\n| train/                |          |\n|    entropy_loss       | -0.324   |\n|    explained_variance | 0.00142  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 999      |\n|    policy_loss        | -3.38    |\n|    value_loss         | 8.31     |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 157       |\n|    ep_rew_mean        | -300      |\n| time/                 |           |\n|    fps                | 335       |\n|    iterations         | 1100      |\n|    time_elapsed       | 16        |\n|    total_timesteps    | 5500      |\n| train/                |           |\n|    entropy_loss       | -0.881    |\n|    explained_variance | -0.000152 |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1099      |\n|    policy_loss        | 2.38      |\n|    value_loss         | 20.6      |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 164      |\n|    ep_rew_mean        | -291     |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 1200     |\n|    time_elapsed       | 17       |\n|    total_timesteps    | 6000     |\n| train/                |          |\n|    entropy_loss       | -0.757   |\n|    explained_variance | 0.000136 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1199     |\n|    policy_loss        | -4.38    |\n|    value_loss         | 78.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 163      |\n|    ep_rew_mean        | -269     |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 1300     |\n|    time_elapsed       | 19       |\n|    total_timesteps    | 6500     |\n| train/                |          |\n|    entropy_loss       | -0.822   |\n|    explained_variance | -0.00024 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1299     |\n|    policy_loss        | -0.365   |\n|    value_loss         | 2.47     |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 169       |\n|    ep_rew_mean        | -264      |\n| time/                 |           |\n|    fps                | 334       |\n|    iterations         | 1400      |\n|    time_elapsed       | 20        |\n|    total_timesteps    | 7000      |\n| train/                |           |\n|    entropy_loss       | -0.867    |\n|    explained_variance | -3.47e-05 |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 1399      |\n|    policy_loss        | 1.92      |\n|    value_loss         | 3.71      |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 173      |\n|    ep_rew_mean        | -256     |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 1500     |\n|    time_elapsed       | 22       |\n|    total_timesteps    | 7500     |\n| train/                |          |\n|    entropy_loss       | -1.01    |\n|    explained_variance | 0.000131 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1499     |\n|    policy_loss        | 1.08     |\n|    value_loss         | 7.27     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 174      |\n|    ep_rew_mean        | -249     |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 1600     |\n|    time_elapsed       | 23       |\n|    total_timesteps    | 8000     |\n| train/                |          |\n|    entropy_loss       | -0.66    |\n|    explained_variance | 0.0799   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1599     |\n|    policy_loss        | 6.28     |\n|    value_loss         | 49.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 174      |\n|    ep_rew_mean        | -249     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 1700     |\n|    time_elapsed       | 25       |\n|    total_timesteps    | 8500     |\n| train/                |          |\n|    entropy_loss       | -1.07    |\n|    explained_variance | -0.129   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1699     |\n|    policy_loss        | -0.0854  |\n|    value_loss         | 0.0347   |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 187      |\n|    ep_rew_mean        | -231     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 1800     |\n|    time_elapsed       | 27       |\n|    total_timesteps    | 9000     |\n| train/                |          |\n|    entropy_loss       | -0.717   |\n|    explained_variance | -0.02    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1799     |\n|    policy_loss        | 0.743    |\n|    value_loss         | 3.73     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 192      |\n|    ep_rew_mean        | -228     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 1900     |\n|    time_elapsed       | 28       |\n|    total_timesteps    | 9500     |\n| train/                |          |\n|    entropy_loss       | -0.688   |\n|    explained_variance | 0.0196   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1899     |\n|    policy_loss        | -3.61    |\n|    value_loss         | 37.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 192      |\n|    ep_rew_mean        | -223     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 2000     |\n|    time_elapsed       | 30       |\n|    total_timesteps    | 10000    |\n| train/                |          |\n|    entropy_loss       | -0.439   |\n|    explained_variance | -0.0117  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1999     |\n|    policy_loss        | -1.08    |\n|    value_loss         | 3.33     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 195      |\n|    ep_rew_mean        | -216     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 2100     |\n|    time_elapsed       | 31       |\n|    total_timesteps    | 10500    |\n| train/                |          |\n|    entropy_loss       | -0.105   |\n|    explained_variance | -0.0434  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2099     |\n|    policy_loss        | -4.15    |\n|    value_loss         | 21.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 197      |\n|    ep_rew_mean        | -213     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 2200     |\n|    time_elapsed       | 33       |\n|    total_timesteps    | 11000    |\n| train/                |          |\n|    entropy_loss       | -0.519   |\n|    explained_variance | -0.0266  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2199     |\n|    policy_loss        | -3.16    |\n|    value_loss         | 26.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 199      |\n|    ep_rew_mean        | -209     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 2300     |\n|    time_elapsed       | 34       |\n|    total_timesteps    | 11500    |\n| train/                |          |\n|    entropy_loss       | -0.671   |\n|    explained_variance | -0.00755 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2299     |\n|    policy_loss        | 4.66     |\n|    value_loss         | 123      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 200      |\n|    ep_rew_mean        | -205     |\n| time/                 |          |\n|    fps                | 332      |\n|    iterations         | 2400     |\n|    time_elapsed       | 36       |\n|    total_timesteps    | 12000    |\n| train/                |          |\n|    entropy_loss       | -0.767   |\n|    explained_variance | -0.00209 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2399     |\n|    policy_loss        | 0.656    |\n|    value_loss         | 2.24     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 201      |\n|    ep_rew_mean        | -205     |\n| time/                 |          |\n|    fps                | 333      |\n|    iterations         | 2500     |\n|    time_elapsed       | 37       |\n|    total_timesteps    | 12500    |\n| train/                |          |\n|    entropy_loss       | -0.995   |\n|    explained_variance | 0.0264   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2499     |\n|    policy_loss        | -7.48    |\n|    value_loss         | 145      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 203      |\n|    ep_rew_mean        | -205     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 2600     |\n|    time_elapsed       | 39       |\n|    total_timesteps    | 13000    |\n| train/                |          |\n|    entropy_loss       | -0.608   |\n|    explained_variance | 0.207    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2599     |\n|    policy_loss        | 0.074    |\n|    value_loss         | 0.0129   |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 210       |\n|    ep_rew_mean        | -197      |\n| time/                 |           |\n|    fps                | 330       |\n|    iterations         | 2700      |\n|    time_elapsed       | 40        |\n|    total_timesteps    | 13500     |\n| train/                |           |\n|    entropy_loss       | -0.721    |\n|    explained_variance | -0.000163 |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 2699      |\n|    policy_loss        | -3.78     |\n|    value_loss         | 44.6      |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 210      |\n|    ep_rew_mean        | -193     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 2800     |\n|    time_elapsed       | 42       |\n|    total_timesteps    | 14000    |\n| train/                |          |\n|    entropy_loss       | -0.633   |\n|    explained_variance | -0.0126  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2799     |\n|    policy_loss        | 3.78     |\n|    value_loss         | 22.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 214      |\n|    ep_rew_mean        | -187     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 2900     |\n|    time_elapsed       | 43       |\n|    total_timesteps    | 14500    |\n| train/                |          |\n|    entropy_loss       | -0.662   |\n|    explained_variance | 0.26     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2899     |\n|    policy_loss        | -1.49    |\n|    value_loss         | 4.79     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 216      |\n|    ep_rew_mean        | -184     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 3000     |\n|    time_elapsed       | 45       |\n|    total_timesteps    | 15000    |\n| train/                |          |\n|    entropy_loss       | -0.502   |\n|    explained_variance | 0.752    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2999     |\n|    policy_loss        | 0.045    |\n|    value_loss         | 3.47     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 218      |\n|    ep_rew_mean        | -185     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 3100     |\n|    time_elapsed       | 47       |\n|    total_timesteps    | 15500    |\n| train/                |          |\n|    entropy_loss       | -0.68    |\n|    explained_variance | 0.0885   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3099     |\n|    policy_loss        | -2.18    |\n|    value_loss         | 14.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 218      |\n|    ep_rew_mean        | -184     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 3200     |\n|    time_elapsed       | 48       |\n|    total_timesteps    | 16000    |\n| train/                |          |\n|    entropy_loss       | -0.615   |\n|    explained_variance | -0.669   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3199     |\n|    policy_loss        | 1.59     |\n|    value_loss         | 18       |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 222      |\n|    ep_rew_mean        | -178     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 3300     |\n|    time_elapsed       | 50       |\n|    total_timesteps    | 16500    |\n| train/                |          |\n|    entropy_loss       | -0.53    |\n|    explained_variance | -0.00905 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3299     |\n|    policy_loss        | 10.6     |\n|    value_loss         | 242      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 221      |\n|    ep_rew_mean        | -175     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 3400     |\n|    time_elapsed       | 51       |\n|    total_timesteps    | 17000    |\n| train/                |          |\n|    entropy_loss       | -0.382   |\n|    explained_variance | -0.0865  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3399     |\n|    policy_loss        | -8.49    |\n|    value_loss         | 1.9e+03  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 220      |\n|    ep_rew_mean        | -172     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 3500     |\n|    time_elapsed       | 52       |\n|    total_timesteps    | 17500    |\n| train/                |          |\n|    entropy_loss       | -0.449   |\n|    explained_variance | -1.73    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3499     |\n|    policy_loss        | -1.03    |\n|    value_loss         | 40.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 220      |\n|    ep_rew_mean        | -169     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 3600     |\n|    time_elapsed       | 54       |\n|    total_timesteps    | 18000    |\n| train/                |          |\n|    entropy_loss       | -0.242   |\n|    explained_variance | 0.133    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3599     |\n|    policy_loss        | -0.322   |\n|    value_loss         | 33.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 221      |\n|    ep_rew_mean        | -169     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 3700     |\n|    time_elapsed       | 55       |\n|    total_timesteps    | 18500    |\n| train/                |          |\n|    entropy_loss       | -0.523   |\n|    explained_variance | 0.616    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3699     |\n|    policy_loss        | -1.4     |\n|    value_loss         | 2.06     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 221      |\n|    ep_rew_mean        | -169     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 3800     |\n|    time_elapsed       | 57       |\n|    total_timesteps    | 19000    |\n| train/                |          |\n|    entropy_loss       | -0.683   |\n|    explained_variance | -0.42    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3799     |\n|    policy_loss        | 0.835    |\n|    value_loss         | 2.85     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 219      |\n|    ep_rew_mean        | -165     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 3900     |\n|    time_elapsed       | 58       |\n|    total_timesteps    | 19500    |\n| train/                |          |\n|    entropy_loss       | -1.26    |\n|    explained_variance | -0.065   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3899     |\n|    policy_loss        | -3.03    |\n|    value_loss         | 24.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 221      |\n|    ep_rew_mean        | -164     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 4000     |\n|    time_elapsed       | 60       |\n|    total_timesteps    | 20000    |\n| train/                |          |\n|    entropy_loss       | -0.335   |\n|    explained_variance | -0.695   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3999     |\n|    policy_loss        | 7.84     |\n|    value_loss         | 190      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 220      |\n|    ep_rew_mean        | -163     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 4100     |\n|    time_elapsed       | 61       |\n|    total_timesteps    | 20500    |\n| train/                |          |\n|    entropy_loss       | -0.657   |\n|    explained_variance | -0.581   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4099     |\n|    policy_loss        | 4.55     |\n|    value_loss         | 78       |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 222      |\n|    ep_rew_mean        | -158     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 4200     |\n|    time_elapsed       | 63       |\n|    total_timesteps    | 21000    |\n| train/                |          |\n|    entropy_loss       | -0.288   |\n|    explained_variance | 0.306    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4199     |\n|    policy_loss        | -0.361   |\n|    value_loss         | 30.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 223      |\n|    ep_rew_mean        | -157     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 4300     |\n|    time_elapsed       | 65       |\n|    total_timesteps    | 21500    |\n| train/                |          |\n|    entropy_loss       | -0.676   |\n|    explained_variance | -1.33    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4299     |\n|    policy_loss        | 9.12     |\n|    value_loss         | 283      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 222      |\n|    ep_rew_mean        | -158     |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 4400     |\n|    time_elapsed       | 66       |\n|    total_timesteps    | 22000    |\n| train/                |          |\n|    entropy_loss       | -0.591   |\n|    explained_variance | 0.0348   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4399     |\n|    policy_loss        | -4.84    |\n|    value_loss         | 90.8     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 222      |\n|    ep_rew_mean        | -153     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 4500     |\n|    time_elapsed       | 68       |\n|    total_timesteps    | 22500    |\n| train/                |          |\n|    entropy_loss       | -0.592   |\n|    explained_variance | -0.78    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4499     |\n|    policy_loss        | 8.04     |\n|    value_loss         | 136      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 225      |\n|    ep_rew_mean        | -144     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 4600     |\n|    time_elapsed       | 69       |\n|    total_timesteps    | 23000    |\n| train/                |          |\n|    entropy_loss       | -0.712   |\n|    explained_variance | -0.187   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4599     |\n|    policy_loss        | 1.57     |\n|    value_loss         | 9.94     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 228      |\n|    ep_rew_mean        | -142     |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 4700     |\n|    time_elapsed       | 71       |\n|    total_timesteps    | 23500    |\n| train/                |          |\n|    entropy_loss       | -0.486   |\n|    explained_variance | 0.302    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4699     |\n|    policy_loss        | 7.51     |\n|    value_loss         | 191      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 228      |\n|    ep_rew_mean        | -135     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 4800     |\n|    time_elapsed       | 72       |\n|    total_timesteps    | 24000    |\n| train/                |          |\n|    entropy_loss       | -0.679   |\n|    explained_variance | -0.0746  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4799     |\n|    policy_loss        | 1.95     |\n|    value_loss         | 12.3     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 230      |\n|    ep_rew_mean        | -122     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 4900     |\n|    time_elapsed       | 73       |\n|    total_timesteps    | 24500    |\n| train/                |          |\n|    entropy_loss       | -0.217   |\n|    explained_variance | 0.983    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4899     |\n|    policy_loss        | -21.9    |\n|    value_loss         | 5.44e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 231      |\n|    ep_rew_mean        | -116     |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 5000     |\n|    time_elapsed       | 75       |\n|    total_timesteps    | 25000    |\n| train/                |          |\n|    entropy_loss       | -0.557   |\n|    explained_variance | -0.0354  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4999     |\n|    policy_loss        | 2.47     |\n|    value_loss         | 99.1     |\n------------------------------------\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766518853
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def record_trained_model(model): \r\n",
        "    episodes = 1\r\n",
        "    env = gym.make(env_name)\r\n",
        "    env = RecordVideo(env, f'{env_name}-{model_name}-video', episode_trigger=lambda _: True)  \r\n",
        "    \r\n",
        "    for episode in range(episodes): \r\n",
        "        observation = env.reset()\r\n",
        "        done = False \r\n",
        "        score = 0 \r\n",
        "        while not done: \r\n",
        "            action, _states = model.predict(observation)\r\n",
        "            observation, reward, done, info = env.step(action)\r\n",
        "            score += reward \r\n",
        "        print(f'Episode={episode+1} Score:{score}')\r\n",
        "    env.close()\r\n",
        "\r\n",
        "    # env = gym.make(env_name)#, render_mode='rgb_array')\r\n",
        "    # env = RecordVideo(env, f'{env_name}-{model_name}-video', episode_trigger=lambda _: True)\r\n",
        "    #env = DummyVecEnv([lambda: env])\r\n",
        "    \r\n",
        "    # observation = env.reset()\r\n",
        "    # for episode in range(episodes): \r\n",
        "    #     state = env.reset()\r\n",
        "    #     done = False \r\n",
        "    #     score = 0 \r\n",
        "    #     while not done: \r\n",
        "    #         action = model.predict(observation)\r\n",
        "    #         observation, reward, done, info = env.step(action)\r\n",
        "    #         score += reward \r\n",
        "    #     print(f'Episode={episode+1} Score:{score}')\r\n",
        "    # env.close()"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766611933
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_trained_model(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/batch/tasks/shared/LS_root/mounts/clusters/gpucomputer/code/Users/fkhan/RL-reserach/LunarLander-v2-a2c-video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n  logger.warn(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Episode=1 Score:-12.484416056075574\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1672766619404
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}